\begin{enumerate}
    \item
        线性优化与非线性优化

        线性优化形如
        \optmodule*{\max}{\bm{c}^\mathrm{T}\bm{x}}{
            &\bm{Ax}\leq\bm{b},
        }
        它的目标函数和约束条件都是线性的.

        非线性优化形如
        \optmodule*{\max}{f(\bm{x})}{
            &g_i(\bm{x})\leq0, \\
            &h_j(\bm{x})=0.
        }
        它的目标函数或约束条件中至少有一个是非线性的.

        线性优化是非线性优化的一种特例，即非线性优化更一般.
    
    \item
        凸优化与非凸优化

        凸优化需要满足三点: (1) 目标函数是凸函数; (2) 约束是凸集; (3) 具有唯一全局最优解.
        非凸优化的目标函数或约束可能是非凸的.

        凸优化是非凸优化的一个特例, 很多非凸问题可以通过凸化转化为凸优化问题.

    \item
        光滑优化与非光滑优化

        光滑优化中的目标函数可微, 并且通常具有连续的一阶或高阶导数.
        非光滑优化中的目标函数不可微, 如$|x|$在0点处不可导.

        许多非光滑优化可以通过光滑化处理, 使其变成光滑优化.

    \item
        线性化: 通过泰勒展开, 一阶近似等方式, 把非线性问题转化为线性问题, 如
        \begin{equation*}
            f(\bm{x})\approx f(\bm{x}_0)+\nabla^\mathrm{T} f(\bm{x}_0)(\bm{x}-\bm{x}_0)
        \end{equation*}
        可将非线性函数近似成线性函数.

        凸化: 凸化是使非凸优化问题变成凸优化问题的常见方法, 如松弛, 变量变换, 凸包逼近等.

        光滑化: 通过引入平滑函数逼近非光滑函数, 如$\sigma$平滑
        \begin{equation*}
            H(x)=
            \begin{cases}
                \frac{1}{2}x^2, & |x|\leq\sigma, \\
                \sigma\left(|x|-\frac{1}{2}\sigma\right), & |x|>\sigma,
            \end{cases}
        \end{equation*}
        或Softmax近似等.

    \item
        近年来, 随着深度学习的快速发展, 非凸优化也更加受到重视.

        神经网络的损失函数通常是非凸的, 且神经网络权重空间往往复杂, 存在多个局部最优和鞍点, 在训练过程使用的梯度下降方法并不保证找到全局最优解.
        但近年来的研究发现, 即使深度学习的优化问题是非凸的, 实践中仍然可以找到可接受的解决方案, 比如有研究表明, 大规模神经网络的局部最优点往往表现良好.
\end{enumerate}
