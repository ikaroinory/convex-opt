\section{结论}

本次实验系统地梳理了多种优化方法的原理与特性, 并通过PyTorch框架实现了这些算法.
使用Rosenbrock函数对各法进行了性能评估, 从\cref{figure:random search}至\cref{figure:krylov}与\cref{table:result}中可以深入分析各法的表现.

从各法的最优值收敛曲线来看, 大部分方法在迭代初期能够迅速降低目标函数值.
然而, 具体收敛速度随着时间推移呈现出明显差异.
例如随机搜索法在初期表现较为波动, 但随着迭代次数增加, 其目标函数值逐渐稳定并接近最优值.
而梯度下降法与次梯度下降法在初期的收敛速度相对缓慢, 但随着迭代次数增加, 逐渐展现出稳定的下降趋势, 最终逼近最优解.

在最优点收敛路径方面, 不同法呈现出独特的路径特征.
共轭方向法与共轭梯度法表现出较为直接的收敛路径, 能够在较少的迭代步骤中找到最优解附近的位置.
相较于一阶方法, BFGS法通过近似二阶信息, 在迭代过程中展现出更快的收敛速度, 其路径也更为平滑.

从\cref{table:result}中可以看出, Newton法在所有方法中所需的迭代轮次最少, 这得益于其充分利用了二阶导数信息, 从而实现对目标函数的二次逼近, 展现出极高的收敛效率.
阻尼Newton法虽然在一定程度上增加了计算复杂度, 但通过阻尼因子的引入, 有效避免了Newton法可能出现的发散问题, 同时保持了较快的收敛速度.

在SGD法的表现中, 我们可以观察到其在大规模样本问题中的潜力.
虽然在本次实验中使用的是固定的目标函数, 但SGD通过对随机样本的更新策略, 在迭代过程中依然能够逐步接近最优解, 显示出其在处理大规模数据集时的适用性.

ADMM法主要用于分布式优化和带有约束条件的问题.
从实验结果来看, ADMM在处理具有分离结构的优化问题时, 能够有效地分解原问题, 通过交替更新变量和乘子, 最终实现对原问题的求解.
其收敛路径表现出一定的波动性, 但在迭代过程中仍能逐步逼近最优解.

从整体的实验结果来看, 各法均能够有效地在一定迭代次数内将目标函数值降至接近最优值的水平.
但从收敛速度和迭代轮次的角度分析, Newton法及其改进形式(阻尼Newton法)展现出明显的优势.
然而, 这种优势的取得是以计算二阶导数信息为代价的, 因此在实际应用中需要权衡计算复杂度和收敛速度之间的关系.

在面对高维大规模问题时, Krylov子空间方法等迭代策略展现出了其独特的优势.
这种基于子空间投影的优化策略, 能够在较低的计算成本下逐步逼近最优解, 为高维问题的求解提供了有效的途径.
从随机搜索法的表现中, 我们也可以得到一些启发.
尽管其收敛速度相对较慢, 但在某些特定场景下, 例如目标函数难以求导或导数信息不可靠时, 随机搜索法作为一种无需梯度信息的优化方法, 仍具有其独特的应用价值.
